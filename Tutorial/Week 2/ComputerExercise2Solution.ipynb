{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kknqZZRPtK0t"
   },
   "source": [
    "# Advanced Risk Management – Assignment 1\n",
    "\n",
    "**Deadline**:  February 20, 13.00h.\n",
    "\n",
    "| |Name |Student number|Email|\n",
    "|:-|:----|:-------------|:----|\n",
    "|1.|  |        |     |\n",
    "|2.|  |        |     |\n",
    "\n",
    "****Hand in the following via Canvas****:\n",
    "* Your notebook.\n",
    "* A (printed) pdf version of your notebook. In Google Colab, this is most conveniently done in the Chrome browser, and then using the **`File` -> `Print`** menu option; you may have to print in landscape mode to make sure that everything appears in the pdf.\n",
    "\n",
    "****Notes****:\n",
    "* The assignment is part of the examination, so the usual rules regarding plagiarism and fraud apply.\n",
    "* Before submitting your work, click on **`Runtime`-> `Restart and run all ...`** and verify that your notebook produces the desired results and does not error.\n",
    "\n",
    "**Declaration of Originality**: We whose names are given under 1. and 2. above declare that these solutions are solely our own work, and that we have not made these solutions available to any other student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyVcGIhkzW1b"
   },
   "source": [
    "## Introduction\n",
    "The file `RV_data.xlsx` contains daily data (January 2000 – February 2020, or a sub-period), for a number of international stock market indices, on the open-to-close log-return R (measured as percentage) and the daily realized variance RV (obtained from 5-minute returns).\n",
    "A list of the included indices is given on the website of the data provider, see\n",
    "https://realized.oxford-man.ox.ac.uk/data/assets (in the Excel file, the leading '.' has been removed from the symbol; e.g. `FCHI` instead of `.FCHI`). In this assignment, you are asked to estimate, test and compare two GARCH models for one of the indices in terms of their in-sample fit and their out-of-sample forecast quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4v62A5fYBUgU"
   },
   "source": [
    "## Question 1: Load and display data\n",
    "First, install and import the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2b_YQlgBYt9"
   },
   "outputs": [],
   "source": [
    "# !pip install arch             \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from arch import arch_model\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# If you cannot import arch, then import the previous version as follows:\n",
    "# !pip install arch==4.6.0 # this is an old version of 'arch' package, helps to avoid an error\n",
    "# from arch import arch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1m5W_Ks1CNUX"
   },
   "source": [
    "Next, import the data and obtain the returns for one chosen index. Uncomment and adapt the lines necessary to mount the drive and change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDThtdbZz_KU"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'close_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'close_price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13836/3793229424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                              \u001b[1;31m# change 'ABC' to chosen index symbol, e.g., 'FCHI'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Define returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close_price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close_price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate close-to-close returns (not that in lab 1 we used different formula)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# drop N/A entries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'close_price'"
     ]
    }
   ],
   "source": [
    "# Recall that there are two ways of downloading data:\n",
    "\n",
    "# 1) \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# path = '/content/drive/My Drive/Colab Notebooks/ARM/Assignment1/'    # change path to your working directory\n",
    "# os.chdir(path)\n",
    "# 2) Direcly read imported 'Excel file'\n",
    "# Note that I use raw data: you already have adjusted data (scaled), so you don't need to define returns, scale them and scale RV; eveyrything is done for you!\n",
    "# df = pd.read_excel(r'C:\\Users\\Jinhyun\\Documents\\GitHub\\Advanced-Risk-Management\\Tutorial\\Week 2\\RV_data.xlsx')\n",
    "\n",
    "# df['Date'] = df['Date'].str[0:10] # remove irrelevant parts of date string, this helps to get rid of the seconds in the data frame\n",
    "# df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d') # transform date strings to datetime \n",
    "# df = df.set_index(['Date'])  # set index of DataFrame to the date column (this allows you to refer to rows of your DataFrame using just time index, it is convenient)\n",
    "# sel = df['Symbol']=='.AEX'   # Boolean array to select index;\n",
    "#                              # change 'ABC' to chosen index symbol, e.g., 'FCHI'\n",
    "# # Define returns   \n",
    "# df['R'] = np.log(df['close_price']) - np.log((df['close_price']).shift(1)) # calculate close-to-close returns (not that in lab 1 we used different formula)\n",
    "# df.dropna() # drop N/A entries\n",
    "\n",
    "# Select particular returns and RV\n",
    "R = 100 * (df['R'].loc[sel])['2000-01-04':] # transform returns to percentages (recommended for GARCH package) and remove first row from returns \n",
    "RV = 100**2 * (df['rv5'].loc[sel])['2000-01-04':] # transform realized variances as well\n",
    "\n",
    "# Define new Data Frame: first column - return; second - RV\n",
    "df2 = pd.DataFrame({'R': R,'RV': RV}) # create DateFrame of transformed returns and realized variances\n",
    "\n",
    "\n",
    "# Note on the adjustement for close-to-close return:\n",
    "# 1) We compute r_t using closing prices, this means that r_t is return on an investment buying asset at the end of yesterday and selling at the end of today\n",
    "# 2) However, assets (stocks, foreign exchange) are traded !during! the day. This means that high-frequency prices are available between 'open' and 'close'.\n",
    "# 3) Availability of high-frequency prices implies that the realised variance (sum of squared high-frequency prices) only accounts for information between 'open' and 'close'\n",
    "# Hence, it does not account for information between 'close' and 'open' (overnight information)\n",
    "# 4) In other words, the realised variance (RV) is directly related to the volatility of open-to-close returns, but captures only a fraction of the volatility of close-to-close returns.\n",
    "# 5) This means that we have to re-scale RV to account for these overnight news (close-to-open news), because once again, our returns are close-to-close!\n",
    "# df2['RV'] = df2['RV'] / (df2['RV'].mean() / (df2['R']**2).mean()) # rescale RV to account for close-to-open news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\Jinhyun\\Documents\\GitHub\\Advanced-Risk-Management\\Tutorial\\Week 2\\RV_data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13836/2590337124.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "df['Date'] = df['Date'].str[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do slice indexing on Int64Index with these indexers [2000-01-04] of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13836/2497722406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Select particular returns and RV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2000-01-04'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# transform returns to percentages (recommended for GARCH package) and remove first row from returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mRV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rv5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2000-01-04'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# transform realized variances as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[1;31m# _convert_slice_indexer to determine if this slice is positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;31m#  or label based, and if the latter, convert to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m             \u001b[0mslobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\numeric.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   3717\u001b[0m             \"\"\"\n\u001b[0;32m   3718\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_index_slice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3719\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3721\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_validate_indexer\u001b[1;34m(self, form, key, kind)\u001b[0m\n\u001b[0;32m   5717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5718\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5719\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot do slice indexing on Int64Index with these indexers [2000-01-04] of type str"
     ]
    }
   ],
   "source": [
    "\n",
    "sel = df['Symbol']=='.AEX' \n",
    "\n",
    "# Select particular returns and RV\n",
    "R = 100 * (df['R'].loc[sel])['2000-01-04':] # transform returns to percentages (recommended for GARCH package) and remove first row from returns \n",
    "RV = 100**2 * (df['rv5'].loc[sel])['2000-01-04':] # transform realized variances as well\n",
    "\n",
    "# Define new Data Frame: first column - return; second - RV\n",
    "df2 = pd.DataFrame({'R': R,'RV': RV}) # create DateFrame of transformed returns and realized variances\n",
    "\n",
    "\n",
    "# Note on the adjustement for close-to-close return:\n",
    "# 1) We compute r_t using closing prices, this means that r_t is return on an investment buying asset at the end of yesterday and selling at the end of today\n",
    "# 2) However, assets (stocks, foreign exchange) are traded !during! the day. This means that high-frequency prices are available between 'open' and 'close'.\n",
    "# 3) Availability of high-frequency prices implies that the realised variance (sum of squared high-frequency prices) only accounts for information between 'open' and 'close'\n",
    "# Hence, it does not account for information between 'close' and 'open' (overnight information)\n",
    "# 4) In other words, the realised variance (RV) is directly related to the volatility of open-to-close returns, but captures only a fraction of the volatility of close-to-close returns.\n",
    "# 5) This means that we have to re-scale RV to account for these overnight news (close-to-open news), because once again, our returns are close-to-close!\n",
    "df2['RV'] = df2['RV'] / (df2['RV'].mean() / (df2['R']**2).mean()) # rescale RV to account for close-to-open news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kityY-O2LB4h"
   },
   "source": [
    "Display a line graph of the returns, and display the autocorrelation function of the returns and of the squared returns. Discuss whether you find the \"stylized facts\" mentioned in the textbook and slides of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8314,
     "status": "ok",
     "timestamp": 1581526519783,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "rZvqwr1FLReZ",
    "outputId": "a78a592e-8b68-4e99-a537-398bb9232b55"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'R' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13836/2317715201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Figure 1: AEX Returns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot_acf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'R' is not defined"
     ]
    }
   ],
   "source": [
    "R.plot()\n",
    "plt.title('Figure 1: AEX Returns')\n",
    "plt.legend(['R'])\n",
    "plt.show()\n",
    "plot_acf(R, lags=np.arange(1,51))\n",
    "plt.title('Figure 2: Autocorrelation function of returns')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()\n",
    "plot_acf(R**2, lags=np.arange(1,51))\n",
    "plt.title('Figure 3: Autocorrelation function of squared returns')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60C90oALLThc"
   },
   "source": [
    "Discussion of results:\n",
    "* The returns in Figure 1 clearly display volatility clustering, which is confirmed by Figure 3 (autocorrelation function of squared returns).\n",
    "* The mean return seems close to zero; all this is in agreement with the stylized facts.\n",
    "* The autocorrelations in the returns are small (less than 0.05 in absolute value), but we do see some significant autocorrelations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63Thc8rvtK0y"
   },
   "source": [
    "## Question 2: Fitting a symmetric GARCH model\n",
    "Estimate and test a GARCH model for the returns, using only data over the sub-period January 2000 – December 2012. For this question, do **not** consider a GJR-GARCH model or any model with an asymmetric NIC (see next question), but focus on standard GARCH($p,q$) models. Display and discuss the estimation output, and test the model for the absence of volatility clustering in the standardized residuals (shocks) $\\hat{z}_t$. If you try out various GARCH models, only report discuss the results on the final model.\n",
    "\n",
    "Note: estimation over a sub-sample using the ARCH package can be done by specifying `last_obs =` in the `.fit` function; see https://arch.readthedocs.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9161,
     "status": "ok",
     "timestamp": 1581526520640,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "FFhK9P4kvOzh",
    "outputId": "97da3df8-9017-4861-921c-fd95fc8f2967"
   },
   "outputs": [],
   "source": [
    "split_date = dt.datetime(2013,1,2) # define last observation in your sub-sample\n",
    "am1 = arch_model(R,p=1,o=0,q=1)\n",
    "res1 = am1.fit(last_obs=split_date, disp='off')\n",
    "print(res1.summary())\n",
    "\n",
    "# Constract standardized residuals\n",
    "z1 = res1.resid['2000-01-04':'2012-12-31'] # extract residuals from the model (varepsilon_t), extract only values from sub-sample (rest is NaN)\n",
    "cond_vol1 = res1.conditional_volatility['2000-01-04':'2012-12-31'] # extract conditional volatility\n",
    "z1_stand = z1/cond_vol1 # obtain standardized residuals\n",
    "\n",
    "# Plot standardized residuals\n",
    "z1_stand.plot()\n",
    "plt.title('Figure 4: Standardized shocks GARCH(1,1)')\n",
    "plt.legend(['z'])\n",
    "plt.show()\n",
    "\n",
    "# Diagnostic check (based on standardised residuals): since e_t (see presentation slides) are i.i.d. N(0,1), we should find no autocorrelation in \\hat_e_t, \\hat_e^2_t if the model is correctly specified\n",
    "# Plot autocorrelation function of standardized residuals: to check whether some heteroscedasticity is still there (remaining volatility clustering)\n",
    "plot_acf(z1_stand**2, lags=np.arange(1,51))\n",
    "plt.title('Figure 5: Autocorrelation function of squared shocks, GARCH(1,1)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()\n",
    "\n",
    "# alternative way of plotting standardized residuals + conditional volatility; could serve as a check for manual computation of standardised residuals\n",
    "res1.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLiGCe2TtK1E"
   },
   "source": [
    "Discussion of results:\n",
    "* The estimation results show that $\\hat{\\alpha}$ and $\\hat{\\beta}$ are significantly different from zero, and have the typical values for financial data ($\\hat{\\alpha}\\approx 0.1$, $\\hat{\\alpha}+\\hat{\\beta}$ close to but smaller than 1).\n",
    "* The estimated mean return is significantly different from zero.\n",
    "* The line graph of the shocks, and the autocorrelation function of $\\hat{z}_t^2$ show very little remaining volatility clustering in $\\hat{z}_t$, which suggests that GARCH(1,1) is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkpnWjK_tK1M"
   },
   "source": [
    "## Question 3: Fitting an asymmetric GARCH model\n",
    "(a) Extend the model you obtained above with one or more asymmetric terms (leading to a GJR-GARCH model), and estimate this second model using data over the same sub-period. Analogously to Question 2, display and discuss the estimation output, and test for volatility clustering in $\\hat{z}_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9542,
     "status": "ok",
     "timestamp": 1581526521032,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "TT_mZiVDtK1Q",
    "outputId": "5285b919-555d-464c-9c23-61d844d462e2"
   },
   "outputs": [],
   "source": [
    "am2 = arch_model(R,p=1,o=1,q=1) # Note that change: asymmetric term is equal to 1 ('o=1')\n",
    "res2 = am2.fit(last_obs=split_date, disp='off')\n",
    "print(res2.summary())\n",
    "\n",
    "\n",
    "z2 = res2.resid['2000-01-04':'2012-12-31']\n",
    "cond_vol2 = res1.conditional_volatility['2000-01-04':'2012-12-31']\n",
    "z2_stand = z2/cond_vol2\n",
    "\n",
    "z2_stand.plot()\n",
    "plt.title('Figure 6: Standardized shocks GJR-GARCH(1,1)')\n",
    "plt.legend(['z'])\n",
    "plt.show()\n",
    "\n",
    "plot_acf(z2_stand**2, lags=np.arange(1,51))\n",
    "plt.title('Figure 7: Autocorrelation function of squared shocks, GJR-GARCH(1,1)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSjBCoWCtK1c"
   },
   "source": [
    "Discussion of results:\n",
    "* We see that the estimated asymmetry parameter $\\hat{\\gamma}$ is positive and significantly different from zero.\n",
    "* In this case $\\hat{\\alpha} = 0$, which implies that positive shocks have no effect on future volatility, only negative shocks.\n",
    "* The stationarity condition is now $\\alpha + \\beta + \\gamma/2 < 1$, which is satisfied by the estimates.\n",
    "* The estimated mean return is now negative, and now not significantly different from 0.\n",
    "* The shocks and the autocorrelation function of the squared shocks are almost the same as in the symmetric GARCH model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfyoWylOMMhq"
   },
   "source": [
    "(b) Carry out a likelihood ratio test for the symmetric model of Question 2 (the null hypothesis) against the asymmetric model of Question 3 (the alternative hypothesis). Obtain the p-value of the test from the `scipy.stats` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9534,
     "status": "ok",
     "timestamp": 1581526521036,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "ACcEa4ZeNGNy",
    "outputId": "71c5bb89-1f4b-45dc-8fbc-437910344df5"
   },
   "outputs": [],
   "source": [
    "LR = 2*(res2.loglikelihood - res1.loglikelihood)\n",
    "pval = 1 - stats.chi2.cdf(LR,1)\n",
    "print(\"Likelihood ratio statistic =\", str.format('{0:.4f}', LR),\n",
    "      \"; p.value =\", str.format('{0:.4f}', pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V40yIRHNQnk"
   },
   "source": [
    "Discussion of results:\n",
    "* The likelihood ratio statistic is much larger than the $\\chi_1^2$ $5\\%$ critical value (3.841), and indeed the p-value is essentially 0.\n",
    "* So we reject the null hypothesis, the fit of the asymmetric GJR-GARCH model is significantly better than the symmetric GARCH model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6tQe0TbOf7i"
   },
   "source": [
    "## Question 4: Comparing the out-of-sample fit\n",
    "From the two models estimated above, obtain the out-of-sample (one-step-ahead) conditional variance predictions over the period January 2013 – February 2020. These are based on coefficient estimates from data until 2012, but they use the most recent returns $R_t$ to obtain the predictions for $\\sigma_{t+1}^2$ in the period after 2012 (consult the ARCH package documentation for details).\n",
    "\n",
    "(a) Make a new data-frame containing the two sets of variance predictions, as wel as the realized variance for the same index and over the same sub-period (January 2013 – February 2020). Make a plot of the two predicted volatility series (square root of the predicted variances) in one figure, and discuss similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9987,
     "status": "ok",
     "timestamp": 1581526521497,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "5W8ahhprOfFo",
    "outputId": "c70da26c-3ece-4298-d935-6bdc4a1a7f5d"
   },
   "outputs": [],
   "source": [
    "# Here you obtain forecast for \\sigma^2_{t+1} for t running for indices in the out-of-sample period\n",
    "sigma1sq = res1.forecast(horizon=1).variance.iloc[:,0] # one-step-ahead (horizon = 1) forecast of variance using GARCH model, 'iloc helps to choose particular strings, rows'\n",
    "sigma2sq = res2.forecast(horizon=1).variance.iloc[:,0] # one-step-ahead (horizon = 1) forecast of variance using GJR-GARCH model\n",
    "\n",
    "sigma1sq = sigma1sq['2013-01-01':'2020-02-10'] # Extract only values for out-of-sample period (because values for esimtation sub-sample are NaN, get rid of them)\n",
    "sigma2sq = sigma2sq['2013-01-01':'2020-02-10'] # Extract onlye values for out-of-sample period\n",
    "\n",
    "RV = RV['2013-01-01':'2020-02-10'] # Extract values of realised variance only for out-of-sample period\n",
    "df2 = pd.DataFrame({'RV': RV, 'GARCH': sigma1sq, 'GJRGARCH': sigma2sq}) # Put RV, sigma^2_t from GARCH and sigma^2_t from GJR-GARCH into pandas DataFrame\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "np.sqrt(sigma1sq).plot() # Note that we take sqrt of sigma^2_t, because we want to plot volatility (instead of variance)\n",
    "np.sqrt(sigma2sq).plot() # Obtain volatility for GJR-GARCH model\n",
    "plt.legend(['GARCH','GJR-GARCH'])\n",
    "plt.title('Volatilities from GARCH and GJR-GARCH methods')\n",
    "plt.ylabel('Volatility in %') # This is due to the scaling applied to returns (in % growth) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHynI0j1R9Ua"
   },
   "source": [
    "Discussion of results:\n",
    "* The two predicted volatility series have the same pattern in general, but differ in particular at the high volatility periods.\n",
    "* Most often the GJR-GARCH predictions seem to have the highest peak (suggesting that these peaks were generated by negative returns).\n",
    "* But at the end of 2016, GARCH model shows a peak in volatility which is not matched by the GJR-GARCH model (suggesting that this was generated by positive returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ll_h2p6SAsT"
   },
   "source": [
    "(b) Calculate the mean squared error of the two variance predictions (using the realized variance as the true $\\sigma_{t+1}^2$), and discuss the result; which one of the two models gives the best predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9978,
     "status": "ok",
     "timestamp": 1581526521501,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "AY0kgGeAT4YQ",
    "outputId": "8441838f-1cb2-4007-beaa-1ad36428de82"
   },
   "outputs": [],
   "source": [
    "# We compare our variance prediction (sigma^2_t) with the observed proxy for it, which is RV;\n",
    "# This is because sigma^2_t itself is not observed, we have no realised series for it\n",
    "MSE1 = ((RV-sigma1sq)**2).mean()\n",
    "MSE2 = ((RV-sigma2sq)**2).mean()\n",
    "print(\"MSE     GARCH = \", str.format('{0:.4f}', MSE1))\n",
    "print(\"MSE GJR-GARCH = \", str.format('{0:.4f}', MSE2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PQiY8IiT45I"
   },
   "source": [
    "Discussion of results: The difference is not large (about 1.45%), but the GJR-GARCH has a slightly lower MSE, and hence gives the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9b613jGT62n"
   },
   "source": [
    "(c) As discussed in Sections 4.6.3 and 5.7 of the book, variance forecasts $\\hat{\\sigma}_{t+1}^2$ can be evaluated in the linear regression\n",
    "\n",
    "$RV_{t+1} = b_0 + b_1\\hat{\\sigma}_{t+1}^2 + e_{t+1}$,\n",
    "\n",
    "by testing the two restrictions $b_0=0, b_1=1$ (separately and jointly).\n",
    "Estimate this regression twice: first, using the symmetric GARCH model prediction (Question 2), and next, using the asymmetric GARCH model prediction (Question 3) as explanatory variable. Report the estimation results and the outcome of the $F$-test for $b_0=0, b_1=1$ (use heteroskedasticity-robust standard errors). What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9972,
     "status": "ok",
     "timestamp": 1581526521505,
     "user": {
      "displayName": "Peter Boswijk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCK4DuGDZ-PvbGUUXsBP98wnxWXfzP5yW_lBTDlAg=s64",
      "userId": "02332832236395307447"
     },
     "user_tz": -60
    },
    "id": "MFVpnAS7WgY9",
    "outputId": "9226f2fe-3794-4771-eacd-039dc4eb4827"
   },
   "outputs": [],
   "source": [
    "# Recall that you constructed above the new pandas DataFrame (named 'df2') with three columns: RV, GARCH, GJRGARCH\n",
    "# We refer to 'df2' to run two regressions above\n",
    "\n",
    "# Regression for GARCH model\n",
    "reg1 = smf.ols('RV ~ GARCH', data=df2)\n",
    "resreg1 = reg1.fit(cov_type='HC1') # heteroscedasticity-robust standard errors\n",
    "print(resreg1.summary2())\n",
    "# F-test\n",
    "ftest1 = resreg1.f_test(\"Intercept = 0,GARCH=1\")\n",
    "F1 = ftest1.fvalue[0][0]\n",
    "p1 = ftest1.pvalue\n",
    "print('F-test for b0=0, b1=1: F =', str.format('{0:.4f}', F1),'; p-value =', str.format('{0:.4f}', p1))\n",
    "print('')\n",
    "\n",
    "# Regression for GJR model\n",
    "reg2 = smf.ols('RV ~ GJRGARCH', data=df2)\n",
    "resreg2 = reg2.fit(cov_type='HC1')\n",
    "print(resreg2.summary2())\n",
    "# F-test\n",
    "ftest2 = resreg2.f_test(\"Intercept = 0,GJRGARCH=1\")\n",
    "F2 = ftest2.fvalue[0][0]\n",
    "p2 = ftest2.pvalue\n",
    "print('F-test for b0=0, b1=1: F =', str.format('{0:.4f}', F2),'; p-value =', str.format('{0:.4f}', p2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cV359BPWdeY"
   },
   "source": [
    "Discussion of results:\n",
    "* What can you say about separate tests for b0=0 and b1=1? (Note that by default H0=0, hence for b1 you have to adjust z-statistic, or use the confidence interval). t-test does not rejct b0=0, what can you say about b1=1?\n",
    "* In both regressions, the joint F-test does reject the null hypothesis. \n",
    "* The b1 coefficient for the GJR-GARCH predictions is quite far from 1, suggesting that the GJR-GARCH predictions are too variable.\n",
    "* On the other hand the R-squared is a bit higher for the GJR-GARCH predictions.\n",
    "* The overall conclusion is that both models seem to predict about equally well."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
